{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21925825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2 \n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9072f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import solutions\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# import drawing functions\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#import model\n",
    "pose = mp_pose.Pose(static_image_mode=False,       # Still pictures or continuous video frames\n",
    "            model_complexity=2,            # Select the human pose key point detection model, 0 has poor performance but fast, 2 has good performance but slow, and 1 is in between\n",
    "            smooth_landmarks=True,         # whether to smooth keypoints\n",
    "            min_detection_confidence=0.5,  # confidence threshold\n",
    "            min_tracking_confidence=0.5)   # tracking threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf77017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Custom description of 33 key points of the body\n",
    "def process_frame(img):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "    \n",
    "    img_RGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = pose.process(img_RGB)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "    for i in range(33): # Traverse 33 keypoints\n",
    "        cx = int(results.pose_landmarks.landmark[i].x * w)\n",
    "        cy = int(results.pose_landmarks.landmark[i].y * h)\n",
    "        cz = results.pose_landmarks.landmark[i].z\n",
    "\n",
    "        redius = 5\n",
    "        if i == 0: #tip of the nose\n",
    "            img = cv2.circle(img, (cx,cy), redius, (0,0,255), -1)\n",
    "        elif i in [11,12]: #Shoulder\n",
    "            img = cv2.circle(img, (cx,cy), redius, (223,155,6), -1)\n",
    "        elif i in [23,24]: #hip joint\n",
    "            img = cv2.circle(img, (cx,cy), redius, (1,240,255), -1)\n",
    "        elif i in [13,14]: #elbow\n",
    "            img = cv2.circle(img, (cx,cy), redius, (140,47,240), -1)\n",
    "        elif i in [25,26]: #knee\n",
    "            img = cv2.circle(img, (cx,cy), redius, (0,0,255), -1)\n",
    "        elif i in [15,16,27,28]: #wrist and ankle\n",
    "            img = cv2.circle(img, (cx,cy), redius, (223,155,60), -1)\n",
    "        elif i in [17,19,21]: #left hand\n",
    "            img = cv2.circle(img, (cx,cy), redius, (94,218,121), -1)\n",
    "        elif i in [18,20,22]: #right hand\n",
    "            img = cv2.circle(img, (cx,cy), redius, (16,144,247), -1)\n",
    "        elif i in [27,29,31]: #left foot\n",
    "            img = cv2.circle(img, (cx,cy), redius, (29,123,243), -1)\n",
    "        elif i in [28,30,32]: #right foot\n",
    "            img = cv2.circle(img, (cx,cy), redius, (193,182,255), -1)\n",
    "        elif i in [9,10]: #mouth\n",
    "            img = cv2.circle(img, (cx,cy), redius, (205,235,255), -1)\n",
    "        elif i in [1,2,3,4,5,6,7,8]: #eyes and cheeks\n",
    "            img = cv2.circle(img, (cx,cy), redius, (94,218,121), -1)\n",
    "        else: #Other key points\n",
    "            img = cv2.circle(img, (cx,cy), redius, (0,255,0), -1)\n",
    "\n",
    "#         look_img(img)\n",
    "    else:\n",
    "        scaler = 1\n",
    "        failuer_str = 'NO Person'\n",
    "        img = cv2.putText(img, failuer_str, (25 * scaler, 100 * scaler), cv2.FONT_HERSHEY_SIMPLEX, 1.25 * scaler, 255,0,0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    FPS = 1/(end_time - start_time)\n",
    "    \n",
    "    scaler = 1\n",
    "    img = cv2.putText(img, 'FPS  '+str(int(FPS)), (25 * scaler, 50 * scaler), cv2.FONT_HERSHEY_SIMPLEX, 1.25 * scaler, 223,155,6)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1743d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video frame-by-frame processing code template\n",
    "def generate_video(input_path):\n",
    "    filehead = input_path.split('/')[-1]\n",
    "    output_path = \"out-\" + filehead\n",
    "    \n",
    "    print('Video starts processing',input_path)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_count = 0\n",
    "    while(cap.isOpened()):\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "        if not success:\n",
    "            break\n",
    "    cap.release()\n",
    "    print('The total number of video frames is',frame_count)\n",
    "    \n",
    "    # cv2.namedWindow('Crack Detection and Measurement Video Processing')\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (int(frame_size[0]), int(frame_size[1])))\n",
    "    \n",
    "    # The progress bar is bound to the total number of frames of the video\n",
    "    with tqdm(total=frame_count-1) as pbar:\n",
    "        try:\n",
    "            while(cap.isOpened()):\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    break\n",
    "\n",
    "                # process frame\n",
    "                # frame_path = './temp_frame.png'\n",
    "                # cv2.imwrite(frame_path, frame)\n",
    "                try:\n",
    "                    frame = process_frame(frame)\n",
    "                except:\n",
    "                    print('error')\n",
    "                    pass\n",
    "                \n",
    "                if success == True:\n",
    "                    # cv2.imshow('Video Processing', frame)\n",
    "                    out.write(frame)\n",
    "\n",
    "                    # The progress bar updates one frame\n",
    "                    pbar.update(1)\n",
    "\n",
    "                # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    # break\n",
    "        except:\n",
    "            print('Interrupted')\n",
    "            pass\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    print('Video saved', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d779d98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video starts processing material/material.mp4\n",
      "The total number of video frames is 527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [00:46<00:00, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved out-material.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_video(input_path=\"material/material.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f7d0801bc5f7331f7f6fbbaeb6dd0f557ef8e98c90abd4bdf7e3dceee9e384d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
